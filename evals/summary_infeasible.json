{
  "total_infeasible": 4,
  "infeasible_scenarios": [
    {
      "scenario": "Read reference files in specific order",
      "reasoning": "Instruction #3 requires reading files in a particular order (voice.md -> tone-guide.md -> ai-anti-patterns.md -> process.md -> product.md). This is a process observation that cannot be verified from file outputs alone. The eval sandbox only grades final files, not agent execution logs or tool call order."
    },
    {
      "scenario": "Don't read product.md upfront",
      "reasoning": "Instruction #4 requires scanning product.md without reading it fully, then fetching specific pages during Phase 0. This is a behavioral constraint on how the agent processes files internally, which cannot be observed from output artifacts."
    },
    {
      "scenario": "Ask for slug before starting Phase 0",
      "reasoning": "Instruction #20 requires the agent to interactively ask the author for a slug before proceeding. The eval sandbox does not support interactive back-and-forth with a user. All scenarios provide the slug upfront in the task description instead."
    },
    {
      "scenario": "Phase 4 surgical file editing",
      "reasoning": "Instruction #31 requires the agent to edit the draft file surgically using the Edit tool during revision based on author feedback. Phase 4 is inherently interactive (author gives feedback, agent revises), which cannot be simulated in a one-shot eval. The revision loop requires multiple turns of human-agent interaction."
    }
  ]
}
